{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471e8fe8-bc4c-4104-ad58-dcef01112403",
   "metadata": {},
   "source": [
    "Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9324bb86-bff9-4452-886f-7a7d87fec5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network with keras \n",
    "#import the proper libraries\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffc1d6-ddba-4b36-8e90-76e17ce2b3d7",
   "metadata": {},
   "source": [
    "Load the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6603260d-3c5b-49b1-9783-7e1933419943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a map with x: rows and input variables and y: an output variable.\n",
    "#This data set is about the Pima Indians onset of diabetes. the data will be stored in a 2d-array.\n",
    "#The first dimension will be rows and the second will be columns.\n",
    "\n",
    "#load the data.\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "\n",
    "#You can split the array into two arrays by selecting subsets of columns using the standard NumPy slice operator or “:”. \n",
    "#You can select the first eight columns from index 0 to index 7 via the slice 0:8. \n",
    "#We can then select the output column (the 9th variable) via index 8.\n",
    "\n",
    "# split into input (X) and output (y) variables\n",
    "x = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "#more about array slicing and ranges: https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/\n",
    "\n",
    "\n",
    "#The data should now be loaded into our program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9270bf-7412-4b00-b438-779dbb98c92d",
   "metadata": {},
   "source": [
    "Define the Keras model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba3b9ef-0c60-407a-9fef-7c7f468e91b9",
   "metadata": {},
   "source": [
    "Models in Keras are a sequence of layers.\n",
    "We will be using a sequential model which is a group of linear stacks. And will continue to add layers until we are happy with\n",
    "our network architecture.\n",
    "\n",
    "\n",
    "When creating the first layer it's important to specicy the correct number of input features.\n",
    "In this dataset's case, 8 input variables are used so we should use the input_shape function so it will look like input_shape=(8,).\n",
    "\n",
    "Usually you can find how many layers you need from trial and error.\n",
    "\n",
    "Will be using a fully-connected network structure with three layers.\n",
    "Fully connected layers are defined using the Dense class. \n",
    "You can specify the number of neurons or nodes in the layer as the first argument and the activation function using the activation argument.\n",
    "\n",
    "will be using the rectified linear unit activation function known as ReLU for the first two layers and then use a Sigmoid function for the output layer.\n",
    "ReLU functions increase the performance of our models which is why we use them and sigmoid functions will ensure our network output will be between 0 and 1.\n",
    "\n",
    "\n",
    "To Sum it Up:\n",
    "    \n",
    "    The model expects rows of data with 8 variables (the input_shape=(8,) argument).\n",
    "    The first hidden layer has 12 nodes and uses the relu activation function.\n",
    "    The second hidden layer has 8 nodes and uses the relu activation function.\n",
    "    The output layer has one node and uses the sigmoid activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bcd097a-d25a-45ba-90c9-d5ce732b6868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 15:48:17.753937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#first hidden layer\n",
    "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "#second hidden layer\n",
    "model.add(Dense(8, activation='relu'))\n",
    "#output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72f779b-051b-4307-8468-226508885d23",
   "metadata": {},
   "source": [
    "When we defined the first hidden layer, we actually were performing two actions due to the input_shape declaration. \n",
    "It defines the input/visible layer while also creating the first hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1322b48-a54b-4225-be16-2e8f736e11d9",
   "metadata": {},
   "source": [
    "Compile the Keras model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c7570b-ee17-4a52-8948-eb5be5190479",
   "metadata": {},
   "source": [
    "We will compile the model using tensorflow. In the backend, tensorflow will take the model and use built-in numerical libraries\n",
    "and find the best way to represent the network for training based on your hardware (CPU/GPU/Distributed).\n",
    "\n",
    "When compiling, more property specifications are needed for training to set effective weights to map inputs and outputs in the dataset.\n",
    "\n",
    "The loss function must be specified. This will be used to search for optimized weights for your desired goals with the dataset. In this case, use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as “binary_crossentropy“.\n",
    "\n",
    "We will define the optimizer as the efficient stochastic gradient descent algorithm “adam“. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems.\n",
    "\n",
    "Finally, the metrics argument will be used to collect and report the classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7f0135-aba0-4eec-9fc9-9bc5cdd3409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c24964-5cec-415f-b78b-02264de06c1a",
   "metadata": {},
   "source": [
    "Fit the Keras model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c46b59-a081-4bb3-9573-91de30c7fc06",
   "metadata": {},
   "source": [
    "Now that out model is compiled, we ready to run some data through it.\n",
    "With the data we've loaded, we can now call the fit() function to train our model.\n",
    "\n",
    "Training occurs over epochs, and each epoch is split into batches:\n",
    "\n",
    "    Epoch: One pass through all of the rows in the training dataset.\n",
    "    Batch: One or more samples considered by the model within an epoch before weights are updated.\n",
    "    \n",
    "The model will run a fixed number of iterations over the dataset which we can define using the epochs argument.\n",
    "You must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size, and set using the batch_size argument.\n",
    "\n",
    "These configurations can be changed and optimized through trial and error. You want to run it enough to where it can learn a good mapping of the rows and output data. The model will always have some error but can level out after some point. This is called model convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595a5391-08d1-4524-81c7-c682aff0857d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/77 [==============================] - 1s 2ms/step - loss: 5.0406 - accuracy: 0.5404\n",
      "Epoch 2/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.8423 - accuracy: 0.5456\n",
      "Epoch 3/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.1705 - accuracy: 0.5768\n",
      "Epoch 4/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.9543 - accuracy: 0.6172\n",
      "Epoch 5/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8962 - accuracy: 0.6185\n",
      "Epoch 6/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8170 - accuracy: 0.6328\n",
      "Epoch 7/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7784 - accuracy: 0.6393\n",
      "Epoch 8/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7482 - accuracy: 0.6419\n",
      "Epoch 9/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7304 - accuracy: 0.6562\n",
      "Epoch 10/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7219 - accuracy: 0.6680\n",
      "Epoch 11/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6999 - accuracy: 0.6432\n",
      "Epoch 12/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.6823\n",
      "Epoch 13/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6699 - accuracy: 0.6667\n",
      "Epoch 14/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6422 - accuracy: 0.6732\n",
      "Epoch 15/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6465 - accuracy: 0.6849\n",
      "Epoch 16/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6459 - accuracy: 0.6771\n",
      "Epoch 17/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6271 - accuracy: 0.6771\n",
      "Epoch 18/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6335 - accuracy: 0.6966\n",
      "Epoch 19/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6325 - accuracy: 0.6901\n",
      "Epoch 20/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6376 - accuracy: 0.6615\n",
      "Epoch 21/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6148 - accuracy: 0.6745\n",
      "Epoch 22/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6105 - accuracy: 0.7005\n",
      "Epoch 23/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6138 - accuracy: 0.6849\n",
      "Epoch 24/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6230 - accuracy: 0.6914\n",
      "Epoch 25/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5966 - accuracy: 0.6823\n",
      "Epoch 26/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5923 - accuracy: 0.7031\n",
      "Epoch 27/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5876 - accuracy: 0.6927\n",
      "Epoch 28/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5893 - accuracy: 0.7083\n",
      "Epoch 29/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5925 - accuracy: 0.7044\n",
      "Epoch 30/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.6836\n",
      "Epoch 31/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6094 - accuracy: 0.6888\n",
      "Epoch 32/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5782 - accuracy: 0.7044\n",
      "Epoch 33/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5631 - accuracy: 0.7044\n",
      "Epoch 34/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5727 - accuracy: 0.7044\n",
      "Epoch 35/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5650 - accuracy: 0.6992\n",
      "Epoch 36/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5735 - accuracy: 0.7174\n",
      "Epoch 37/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5656 - accuracy: 0.7018\n",
      "Epoch 38/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.7240\n",
      "Epoch 39/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5817 - accuracy: 0.7109\n",
      "Epoch 40/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5818 - accuracy: 0.6953\n",
      "Epoch 41/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7174\n",
      "Epoch 42/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5590 - accuracy: 0.7096\n",
      "Epoch 43/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5751 - accuracy: 0.7188\n",
      "Epoch 44/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5515 - accuracy: 0.7214\n",
      "Epoch 45/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.7109\n",
      "Epoch 46/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5453 - accuracy: 0.7253\n",
      "Epoch 47/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5582 - accuracy: 0.7057\n",
      "Epoch 48/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5455 - accuracy: 0.7188\n",
      "Epoch 49/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5466 - accuracy: 0.7161\n",
      "Epoch 50/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5510 - accuracy: 0.7188\n",
      "Epoch 51/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5494 - accuracy: 0.7292\n",
      "Epoch 52/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5556 - accuracy: 0.7318\n",
      "Epoch 53/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5511 - accuracy: 0.7292\n",
      "Epoch 54/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5535 - accuracy: 0.7096\n",
      "Epoch 55/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5597 - accuracy: 0.7031\n",
      "Epoch 56/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5443 - accuracy: 0.7227\n",
      "Epoch 57/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5660 - accuracy: 0.7057\n",
      "Epoch 58/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5416 - accuracy: 0.7201\n",
      "Epoch 59/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5433 - accuracy: 0.7227\n",
      "Epoch 60/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.7266\n",
      "Epoch 61/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5318 - accuracy: 0.7266\n",
      "Epoch 62/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5567 - accuracy: 0.7318\n",
      "Epoch 63/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5358 - accuracy: 0.7370\n",
      "Epoch 64/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5302 - accuracy: 0.7331\n",
      "Epoch 65/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5395 - accuracy: 0.7266\n",
      "Epoch 66/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5357 - accuracy: 0.7318\n",
      "Epoch 67/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5350 - accuracy: 0.7422\n",
      "Epoch 68/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5368 - accuracy: 0.7227\n",
      "Epoch 69/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5272 - accuracy: 0.7370\n",
      "Epoch 70/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5594 - accuracy: 0.7174\n",
      "Epoch 71/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5172 - accuracy: 0.7344\n",
      "Epoch 72/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.7214\n",
      "Epoch 73/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5405 - accuracy: 0.7409\n",
      "Epoch 74/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5291 - accuracy: 0.7266\n",
      "Epoch 75/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5495 - accuracy: 0.7279\n",
      "Epoch 76/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5210 - accuracy: 0.7357\n",
      "Epoch 77/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5494 - accuracy: 0.7266\n",
      "Epoch 78/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5289 - accuracy: 0.7396\n",
      "Epoch 79/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5374 - accuracy: 0.7188\n",
      "Epoch 80/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.7422\n",
      "Epoch 81/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5218 - accuracy: 0.7487\n",
      "Epoch 82/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5199 - accuracy: 0.7591\n",
      "Epoch 83/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7435\n",
      "Epoch 84/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5142 - accuracy: 0.7552\n",
      "Epoch 85/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7292\n",
      "Epoch 86/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7461\n",
      "Epoch 87/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5223 - accuracy: 0.7435\n",
      "Epoch 88/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7318\n",
      "Epoch 89/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5275 - accuracy: 0.7396\n",
      "Epoch 90/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5119 - accuracy: 0.7708\n",
      "Epoch 91/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5198 - accuracy: 0.7370\n",
      "Epoch 92/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5081 - accuracy: 0.7409\n",
      "Epoch 93/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.7487\n",
      "Epoch 94/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5079 - accuracy: 0.7435\n",
      "Epoch 95/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5169 - accuracy: 0.7539\n",
      "Epoch 96/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7461\n",
      "Epoch 97/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5479 - accuracy: 0.7396\n",
      "Epoch 98/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5035 - accuracy: 0.7474\n",
      "Epoch 99/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5196 - accuracy: 0.7448\n",
      "Epoch 100/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5206 - accuracy: 0.7565\n",
      "Epoch 101/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7396\n",
      "Epoch 102/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7383\n",
      "Epoch 103/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.7578\n",
      "Epoch 104/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7513\n",
      "Epoch 105/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7669\n",
      "Epoch 106/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.7344\n",
      "Epoch 107/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5033 - accuracy: 0.7630\n",
      "Epoch 108/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7591\n",
      "Epoch 109/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7539\n",
      "Epoch 110/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5014 - accuracy: 0.7539\n",
      "Epoch 111/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.7383\n",
      "Epoch 112/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5039 - accuracy: 0.7461\n",
      "Epoch 113/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.7669\n",
      "Epoch 114/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.7422\n",
      "Epoch 115/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5049 - accuracy: 0.7552\n",
      "Epoch 116/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.7656\n",
      "Epoch 117/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4938 - accuracy: 0.7617\n",
      "Epoch 118/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5062 - accuracy: 0.7500\n",
      "Epoch 119/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5306 - accuracy: 0.7435\n",
      "Epoch 120/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5193 - accuracy: 0.7474\n",
      "Epoch 121/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5066 - accuracy: 0.7487\n",
      "Epoch 122/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5012 - accuracy: 0.7487\n",
      "Epoch 123/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4962 - accuracy: 0.7513\n",
      "Epoch 124/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4965 - accuracy: 0.7604\n",
      "Epoch 125/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5059 - accuracy: 0.7383\n",
      "Epoch 126/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5500 - accuracy: 0.7409\n",
      "Epoch 127/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4979 - accuracy: 0.7695\n",
      "Epoch 128/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5037 - accuracy: 0.7513\n",
      "Epoch 129/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5060 - accuracy: 0.7474\n",
      "Epoch 130/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4881 - accuracy: 0.7513\n",
      "Epoch 131/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4952 - accuracy: 0.7513\n",
      "Epoch 132/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4963 - accuracy: 0.7591\n",
      "Epoch 133/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7630\n",
      "Epoch 134/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.7630\n",
      "Epoch 135/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5140 - accuracy: 0.7526\n",
      "Epoch 136/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4861 - accuracy: 0.7617\n",
      "Epoch 137/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5187 - accuracy: 0.7435\n",
      "Epoch 138/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4976 - accuracy: 0.7591\n",
      "Epoch 139/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5019 - accuracy: 0.7617\n",
      "Epoch 140/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7656\n",
      "Epoch 141/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4914 - accuracy: 0.7591\n",
      "Epoch 142/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.7617\n",
      "Epoch 143/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4870 - accuracy: 0.7734\n",
      "Epoch 144/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5083 - accuracy: 0.7344\n",
      "Epoch 145/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.7578\n",
      "Epoch 146/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5069 - accuracy: 0.7526\n",
      "Epoch 147/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4951 - accuracy: 0.7539\n",
      "Epoch 148/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4789 - accuracy: 0.7682\n",
      "Epoch 149/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4920 - accuracy: 0.7474\n",
      "Epoch 150/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4824 - accuracy: 0.7721\n",
      "Epoch 151/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4761 - accuracy: 0.7721\n",
      "Epoch 152/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4908 - accuracy: 0.7760\n",
      "Epoch 153/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.7682\n",
      "Epoch 154/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4912 - accuracy: 0.7487\n",
      "Epoch 155/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4893 - accuracy: 0.7695\n",
      "Epoch 156/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4869 - accuracy: 0.7604\n",
      "Epoch 157/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7682\n",
      "Epoch 158/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5121 - accuracy: 0.7565\n",
      "Epoch 159/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.7578\n",
      "Epoch 160/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4960 - accuracy: 0.7617\n",
      "Epoch 161/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4701 - accuracy: 0.7773\n",
      "Epoch 162/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4798 - accuracy: 0.7826\n",
      "Epoch 163/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4807 - accuracy: 0.7682\n",
      "Epoch 164/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4768 - accuracy: 0.7760\n",
      "Epoch 165/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4941 - accuracy: 0.7461\n",
      "Epoch 166/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4762 - accuracy: 0.7630\n",
      "Epoch 167/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4991 - accuracy: 0.7552\n",
      "Epoch 168/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4780 - accuracy: 0.7617\n",
      "Epoch 169/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4947 - accuracy: 0.7565\n",
      "Epoch 170/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.7721\n",
      "Epoch 171/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5242 - accuracy: 0.7409\n",
      "Epoch 172/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4851 - accuracy: 0.7487\n",
      "Epoch 173/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4947 - accuracy: 0.7682\n",
      "Epoch 174/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4846 - accuracy: 0.7682\n",
      "Epoch 175/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.7708\n",
      "Epoch 176/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7708\n",
      "Epoch 177/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7708\n",
      "Epoch 178/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4787 - accuracy: 0.7643\n",
      "Epoch 179/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4790 - accuracy: 0.7513\n",
      "Epoch 180/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7865\n",
      "Epoch 181/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7604\n",
      "Epoch 182/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.7617\n",
      "Epoch 183/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.7734\n",
      "Epoch 184/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7786\n",
      "Epoch 185/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4789 - accuracy: 0.7695\n",
      "Epoch 186/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.7682\n",
      "Epoch 187/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4956 - accuracy: 0.7552\n",
      "Epoch 188/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4883 - accuracy: 0.7643\n",
      "Epoch 189/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4817 - accuracy: 0.7721\n",
      "Epoch 190/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.7708\n",
      "Epoch 191/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5000 - accuracy: 0.7539\n",
      "Epoch 192/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4768 - accuracy: 0.7604\n",
      "Epoch 193/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.7669\n",
      "Epoch 194/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4895 - accuracy: 0.7669\n",
      "Epoch 195/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4740 - accuracy: 0.7799\n",
      "Epoch 196/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4750 - accuracy: 0.7643\n",
      "Epoch 197/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7396\n",
      "Epoch 198/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4779 - accuracy: 0.7604\n",
      "Epoch 199/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.7630\n",
      "Epoch 200/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4692 - accuracy: 0.7682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fab7d855ee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=200, batch_size=10)\n",
    "#The reason is the output progress bars during training. You can easily turn these off by setting verbose=0 in the call to the fit() and evaluate() functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74c253-85fa-45ff-95ef-a8179127ec1c",
   "metadata": {},
   "source": [
    "This is where the work either happens on your CPU or GPU based on configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258a742-493c-412b-938d-7af19219cc51",
   "metadata": {},
   "source": [
    "Evaluate the Keras Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08696887-3586-4ff8-9067-358ed1e3ff72",
   "metadata": {},
   "source": [
    "We have now created a neural keras model and can evaluate its performance on the dataset.\n",
    "This will only give you an idea of how well you have modeled the dataset (e.g., train accuracy), \n",
    "but no idea of how well the algorithm might perform on new data. \n",
    "This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "We can evaluate our model with the evaluate() function by passing it the same input and output used to train the model. This will generated scores based on each input and output pair.The first will be the loss of the model on the dataset, and the second will be the accuracy of the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e30322b-7445-4884-a113-3c8f90a770d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.7799\n",
      "Loss: 46.06\n",
      "Accuracy: 77.99\n"
     ]
    }
   ],
   "source": [
    "#The reason is the output progress bars during training. You can easily turn these off by setting verbose=0 in the call to the fit() and evaluate() functions;\n",
    "loss, accuracy = model.evaluate(x, y)\n",
    "print('Loss: %.2f' % (loss*100))\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}